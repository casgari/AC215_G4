{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- [dask-pytorch-ddp example](https://pypi.org/project/dask-pytorch-ddp/)\n",
        "- [Integrated dask and pytorch](https://saturncloud.io/blog/combining-dask-and-py-torch-for-better-faster-transfer-learning/)\n",
        "- [Torch DDP](https://https://pytorch.org/docs/stable/notes/ddp.html)\n",
        "- [More on DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
        "- [Pytorch across dask cluster](https://https://saturncloud.io/docs/examples/python/pytorch/qs-03-pytorch-gpu-dask-single-model/)\n",
        "- [DDP and Output](https://github.com/saturncloud/dask-pytorch-ddp/blob/main/README.md)"
      ],
      "metadata": {
        "id": "_XT6CJqc3TSH"
      },
      "id": "_XT6CJqc3TSH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where are we at right now?\n",
        "We need to figure out how to get results (estimated time, loss data, etc.) through the `rh.process_results()`. Rn it's hard to tell what's going on because I think the `process_results()` function swallows the normal output from `Trainer.train()` method. Would recommend reading \"DDP and Output\" article above if you want to work on this part.\n",
        "\n",
        "We could integrate TFRecords, however, that might change our training function which takes Datasets. We can convert from TFRecords to Datasets if necessary. Luckily, the Torch Dataset structure is similar in effect to TFRecords, and actually only represents the paths to the data itself and not the actual data. The train method uses a data preloader which doesn't actually hold the data in memory, so these things are automatically integrated because we're using the Dataset data type. We should clarify this, maybe with a couple memory tests comparing size of a Dataset variable to the size of the actual file."
      ],
      "metadata": {
        "id": "N29cLLMm36cC"
      },
      "id": "N29cLLMm36cC"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q wandb\n",
        "# !pip install datasets\n",
        "# !pip install seqeval\n",
        "# !pip install evaluate\n",
        "# !pip install datasets transformers==4.28.0\n",
        "# !pip install transformers[torch]\n",
        "# !pip install dask-pytorch-dpp"
      ],
      "metadata": {
        "id": "fy8oVx5T29h8"
      },
      "id": "fy8oVx5T29h8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911b06d3-3f15-405c-924c-910b9e5f498d",
      "metadata": {
        "id": "911b06d3-3f15-405c-924c-910b9e5f498d"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "import time\n",
        "import uuid\n",
        "import datetime\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# Stand\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, TrainerCallback\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "# Dask\n",
        "from dask_pytorch_ddp import dispatch, results\n",
        "from dask.distributed import Client\n",
        "from distributed.worker import logger\n",
        "\n",
        "# W and B\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "ENRCYjxt21F2"
      },
      "id": "ENRCYjxt21F2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "Uj56lrPC2mj3"
      },
      "id": "Uj56lrPC2mj3"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_words_with_corresponding_labels(sample):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "\n",
        "    #truncation=True to specify to truncate sequences at the maximum length\n",
        "    #is_split_into_words = True to specify that our input is already pre-tokenized (e.g., split into words)\n",
        "    tokenized_inputs = tokenizer(sample[\"document\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    #initialize list to store lists of labels for each sample\n",
        "    labels = []\n",
        "\n",
        "    for i, label in enumerate(sample[\"doc_bio_tags\"]):\n",
        "\n",
        "        #map tokens to their respective word\n",
        "        #word_ids() method gets index of the word that each token comes from\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "\n",
        "        #initialize list of labels for each token in a given sample\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "\n",
        "            #set the special tokens, [CLS] and [SEP], to -100.\n",
        "            # we use -100 because it's an index that is ignored in the loss function we will use (cross entropy).\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "\n",
        "            #set labels for tokens\n",
        "            else:\n",
        "                label_ids.append(label2id[label[word_idx]])\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "B4d2maZZ2ofF"
      },
      "id": "B4d2maZZ2ofF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(preds):\n",
        "    logits, labels = preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return all_metrics"
      ],
      "metadata": {
        "id": "cEQXXp3e2pKj"
      },
      "id": "cEQXXp3e2pKj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "39819887-1117-475c-af95-9711c061aa09",
      "metadata": {
        "id": "39819887-1117-475c-af95-9711c061aa09"
      },
      "source": [
        "## Loading in Inspec Dataset with samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "tThliu3q2RtV"
      },
      "id": "tThliu3q2RtV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7929f88-eb56-4d6d-88a9-8c0683a1f49b",
      "metadata": {
        "id": "a7929f88-eb56-4d6d-88a9-8c0683a1f49b"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"midas/inspec\", \"extraction\")\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "tokenized_dataset = dataset.map(tokenize_words_with_corresponding_labels, batched=True)\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#counting how many beginning keywords, middle keywords, and non-keywords there are\n",
        "count_0s = 0\n",
        "count_1s = 0\n",
        "count_2s = 0\n",
        "\n",
        "for listt in tokenized_dataset[\"train\"][\"labels\"]:\n",
        "    count_dict = Counter(listt)\n",
        "    count_0s += count_dict[0]\n",
        "    count_1s += count_dict[1]\n",
        "    count_2s += count_dict[2]\n",
        "\n",
        "#getting weights for weighted cross_entropy\n",
        "max_ = max(count_0s,count_1s,count_2s)\n",
        "weights = [max_/count_0s, max_/count_1s, max_/count_2s]\n",
        "\n",
        "#defining loss function\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\").to(model.device)\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\").to(model.device)\n",
        "        # compute custom loss (suppose one has 3 labels with different weights)\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight= torch.tensor(weights).to(device))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "watGI7N2L2iS"
      },
      "id": "watGI7N2L2iS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new Dask client that makes use of all available GPUs\n",
        "client = Client()"
      ],
      "metadata": {
        "id": "E-DqWdDpN2C7"
      },
      "id": "E-DqWdDpN2C7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A test to see how we can get output from train function\n",
        "\n",
        "# class MyCallback(TrainerCallback):\n",
        "#     \"A callback that prints a message at the beginning of training\"\n",
        "\n",
        "#     def on_train_begin(self, args, state, control, **kwargs):\n",
        "#         print(\"Starting training\")"
      ],
      "metadata": {
        "id": "pAgY-m-dQcjQ"
      },
      "id": "pAgY-m-dQcjQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = uuid.uuid4().hex\n",
        "rh = results.DaskResultsHandler(key)\n",
        "\n",
        "# Define the training loop\n",
        "def train(model_checkpoint, dataset, data_collator, compute_metrics,  tokenizer):\n",
        "\n",
        "  batch_size = 8\n",
        "  learning_rate=4e-6\n",
        "\n",
        "  epochs = 1\n",
        "  model_name = model_checkpoint.split(\"/\")[-1]\n",
        "  args = TrainingArguments(\n",
        "      f\"{model_checkpoint}_finetuned_keyword_extract\",\n",
        "      evaluation_strategy = \"epoch\",\n",
        "      logging_strategy = 'epoch',\n",
        "      learning_rate=learning_rate,\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      per_device_eval_batch_size=batch_size,\n",
        "      num_train_epochs= epochs,\n",
        "      lr_scheduler_type='linear',\n",
        "      weight_decay=0.01,\n",
        "      seed=0\n",
        "  )\n",
        "\n",
        "  model_token = AutoModelForTokenClassification.from_pretrained(model_checkpoint,\n",
        "                                                          id2label=id2label,\n",
        "                                                      label2id=label2id)\n",
        "  model = model_token.to(device) #need GPU to train\n",
        "  model = DDP(model, device_ids=[0])\n",
        "\n",
        "\n",
        "  # Initialize a W&B run\n",
        "  wandb.init(\n",
        "      project = 'ppp-keyword-extraction',\n",
        "      config = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"model_name\": model_name\n",
        "      },\n",
        "      name = model_name\n",
        "  )\n",
        "\n",
        "\n",
        "  # Train model\n",
        "  start_time = time.time()\n",
        "  print('hey')\n",
        "\n",
        "  # Uses dataloaders (≈TFRecords)\n",
        "  trainer = CustomTrainer(\n",
        "      model=model,\n",
        "      args=args,\n",
        "      train_dataset=dataset[\"train\"],\n",
        "      eval_dataset=dataset[\"validation\"],\n",
        "      data_collator=data_collator,\n",
        "      compute_metrics=compute_metrics,\n",
        "      tokenizer=tokenizer,\n",
        "      callbacks=[MyCallback],)\n",
        "  trainer.train()\n",
        "\n",
        "\n",
        "  # Update W&B\n",
        "  execution_time = (time.time() - start_time)/60.0\n",
        "  wandb.config.update({\"execution_time\": execution_time})\n",
        "  # Close the W&B run\n",
        "  wandb.run.finish()\n",
        "\n",
        "  return trainer"
      ],
      "metadata": {
        "id": "yf_OYi8AF0cM"
      },
      "id": "yf_OYi8AF0cM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_params = {'model_checkpoint':model_checkpoint,\n",
        "                'dataset':tokenized_dataset,\n",
        "                'data_collator':data_collator,\n",
        "                'compute_metrics':compute_metrics,\n",
        "                'tokenizer':tokenizer\n",
        "}\n",
        "\n",
        "futures = dispatch.run(client, train, **start_params)"
      ],
      "metadata": {
        "id": "GuNK6U8vMUx9"
      },
      "id": "GuNK6U8vMUx9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rh.process_results(\n",
        "    \"/\",\n",
        "    futures,\n",
        "    raise_errors=True)"
      ],
      "metadata": {
        "id": "-KMGfrNxRQve"
      },
      "id": "-KMGfrNxRQve",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "Uj56lrPC2mj3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}