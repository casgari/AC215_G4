{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911b06d3-3f15-405c-924c-910b9e5f498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "#!pip install seqeval\n",
    "#!pip install evaluate\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6818692-af1f-4b2e-b2ea-f73922a3c779",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39819887-1117-475c-af95-9711c061aa09",
   "metadata": {},
   "source": [
    "## Loading in Inspec Dataset with samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7929f88-eb56-4d6d-88a9-8c0683a1f49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples for Keyphrase Extraction\n",
      "\n",
      "Sample from training data split\n",
      "Fields in the sample:  ['id', 'document', 'doc_bio_tags']\n",
      "Tokenized Document:  ['A', 'conflict', 'between', 'language', 'and', 'atomistic', 'information', 'Fred', 'Dretske', 'and', 'Jerry', 'Fodor', 'are', 'responsible', 'for', 'popularizing', 'three', 'well-known', 'theses', 'in', 'contemporary', 'philosophy', 'of', 'mind', ':', 'the', 'thesis', 'of', 'Information-Based', 'Semantics', '-LRB-', 'IBS', '-RRB-', ',', 'the', 'thesis', 'of', 'Content', 'Atomism', '-LRB-', 'Atomism', '-RRB-', 'and', 'the', 'thesis', 'of', 'the', 'Language', 'of', 'Thought', '-LRB-', 'LOT', '-RRB-', '.', 'LOT', 'concerns', 'the', 'semantically', 'relevant', 'structure', 'of', 'representations', 'involved', 'in', 'cognitive', 'states', 'such', 'as', 'beliefs', 'and', 'desires', '.', 'It', 'maintains', 'that', 'all', 'such', 'representations', 'must', 'have', 'syntactic', 'structures', 'mirroring', 'the', 'structure', 'of', 'their', 'contents', '.', 'IBS', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'relations', 'that', 'connect', 'cognitive', 'representations', 'and', 'their', 'parts', 'to', 'their', 'contents', '-LRB-', 'semantic', 'relations', '-RRB-', '.', 'It', 'holds', 'that', 'these', 'relations', 'supervene', 'solely', 'on', 'relations', 'of', 'the', 'kind', 'that', 'support', 'information', 'content', ',', 'perhaps', 'with', 'some', 'help', 'from', 'logical', 'principles', 'of', 'combination', '.', 'Atomism', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'content', 'of', 'simple', 'symbols', '.', 'It', 'holds', 'that', 'each', 'substantive', 'simple', 'symbol', 'possesses', 'its', 'content', 'independently', 'of', 'all', 'other', 'symbols', 'in', 'the', 'representational', 'system', '.', 'I', 'argue', 'that', 'Dretske', \"'s\", 'and', 'Fodor', \"'s\", 'theories', 'are', 'false', 'and', 'that', 'their', 'falsehood', 'results', 'from', 'a', 'conflict', 'IBS', 'and', 'Atomism', ',', 'on', 'the', 'one', 'hand', ',', 'and', 'LOT', ',', 'on', 'the', 'other']\n",
      "Document BIO Tags:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "\n",
      "-----------\n",
      "\n",
      "Sample from validation data split\n",
      "Fields in the sample:  ['id', 'document', 'doc_bio_tags']\n",
      "Tokenized Document:  ['Impact', 'of', 'aviation', 'highway-in-the-sky', 'displays', 'on', 'pilot', 'situation', 'awareness', 'Thirty-six', 'pilots', '-LRB-', '31', 'men', ',', '5', 'women', '-RRB-', 'were', 'tested', 'in', 'a', 'flight', 'simulator', 'on', 'their', 'ability', 'to', 'intercept', 'a', 'pathway', 'depicted', 'on', 'a', 'highway-in-the-sky', '-LRB-', 'HITS', '-RRB-', 'display', '.', 'While', 'intercepting', 'and', 'flying', 'the', 'pathway', ',', 'pilots', 'were', 'required', 'to', 'watch', 'for', 'traffic', 'outside', 'the', 'cockpit', '.', 'Additionally', ',', 'pilots', 'were', 'tested', 'on', 'their', 'awareness', 'of', 'speed', ',', 'altitude', ',', 'and', 'heading', 'during', 'the', 'flight', '.', 'Results', 'indicated', 'that', 'the', 'presence', 'of', 'a', 'flight', 'guidance', 'cue', 'significantly', 'improved', 'flight', 'path', 'awareness', 'while', 'intercepting', 'the', 'pathway', ',', 'but', 'significant', 'practice', 'effects', 'suggest', 'that', 'a', 'guidance', 'cue', 'might', 'be', 'unnecessary', 'if', 'pilots', 'are', 'given', 'proper', 'training', '.', 'The', 'amount', 'of', 'time', 'spent', 'looking', 'outside', 'the', 'cockpit', 'while', 'using', 'the', 'HITS', 'display', 'was', 'significantly', 'less', 'than', 'when', 'using', 'conventional', 'aircraft', 'instruments', '.', 'Additionally', ',', 'awareness', 'of', 'flight', 'information', 'present', 'on', 'the', 'HITS', 'display', 'was', 'poor', '.', 'Actual', 'or', 'potential', 'applications', 'of', 'this', 'research', 'include', 'guidance', 'for', 'the', 'development', 'of', 'perspective', 'flight', 'display', 'standards', 'and', 'as', 'a', 'basis', 'for', 'flight', 'training', 'requirements']\n",
      "Document BIO Tags:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "-----------\n",
      "\n",
      "Sample from test data split\n",
      "Fields in the sample:  ['id', 'document', 'doc_bio_tags']\n",
      "Tokenized Document:  ['A', 'new', 'graphical', 'user', 'interface', 'for', 'fast', 'construction', 'of', 'computation', 'phantoms', 'and', 'MCNP', 'calculations', ':', 'application', 'to', 'calibration', 'of', 'in', 'vivo', 'measurement', 'systems', 'Reports', 'on', 'a', 'new', 'utility', 'for', 'development', 'of', 'computational', 'phantoms', 'for', 'Monte', 'Carlo', 'calculations', 'and', 'data', 'analysis', 'for', 'in', 'vivo', 'measurements', 'of', 'radionuclides', 'deposited', 'in', 'tissues', '.', 'The', 'individual', 'properties', 'of', 'each', 'worker', 'can', 'be', 'acquired', 'for', 'a', 'rather', 'precise', 'geometric', 'representation', 'of', 'his', '-LRB-', 'her', '-RRB-', 'anatomy', ',', 'which', 'is', 'particularly', 'important', 'for', 'low', 'energy', 'gamma', 'ray', 'emitting', 'sources', 'such', 'as', 'thorium', ',', 'uranium', ',', 'plutonium', 'and', 'other', 'actinides', '.', 'The', 'software', 'enables', 'automatic', 'creation', 'of', 'an', 'MCNP', 'input', 'data', 'file', 'based', 'on', 'scanning', 'data', '.', 'The', 'utility', 'includes', 'segmentation', 'of', 'images', 'obtained', 'with', 'either', 'computed', 'tomography', 'or', 'magnetic', 'resonance', 'imaging', 'by', 'distinguishing', 'tissues', 'according', 'to', 'their', 'signal', '-LRB-', 'brightness', '-RRB-', 'and', 'specification', 'of', 'the', 'source', 'and', 'detector', '.', 'In', 'addition', ',', 'a', 'coupling', 'of', 'individual', 'voxels', 'within', 'the', 'tissue', 'is', 'used', 'to', 'reduce', 'the', 'memory', 'demand', 'and', 'to', 'increase', 'the', 'calculational', 'speed', '.', 'The', 'utility', 'was', 'tested', 'for', 'low', 'energy', 'emitters', 'in', 'plastic', 'and', 'biological', 'tissues', 'as', 'well', 'as', 'for', 'computed', 'tomography', 'and', 'magnetic', 'resonance', 'imaging', 'scanning', 'information']\n",
      "Document BIO Tags:  ['O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I']\n",
      "\n",
      "-----------\n",
      "\n",
      "Type of each dataset: <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"midas/inspec\", \"extraction\")\n",
    "\n",
    "\n",
    "print(\"Samples for Keyphrase Extraction\\n\")\n",
    "\n",
    "# sample from the train split\n",
    "print(\"Sample from training data split\")\n",
    "train_sample = dataset[\"train\"][0]\n",
    "print(\"Fields in the sample: \", [key for key in train_sample.keys()])\n",
    "print(\"Tokenized Document: \", train_sample[\"document\"])\n",
    "print(\"Document BIO Tags: \", train_sample[\"doc_bio_tags\"])\n",
    "print(\"\\n-----------\\n\")\n",
    "\n",
    "# sample from the validation split\n",
    "print(\"Sample from validation data split\")\n",
    "validation_sample = dataset[\"validation\"][0]\n",
    "print(\"Fields in the sample: \", [key for key in validation_sample.keys()])\n",
    "print(\"Tokenized Document: \", validation_sample[\"document\"])\n",
    "print(\"Document BIO Tags: \", validation_sample[\"doc_bio_tags\"])\n",
    "print(\"\\n-----------\\n\")\n",
    "\n",
    "# sample from the test split\n",
    "print(\"Sample from test data split\")\n",
    "test_sample = dataset[\"test\"][0]\n",
    "print(\"Fields in the sample: \", [key for key in test_sample.keys()])\n",
    "print(\"Tokenized Document: \", test_sample[\"document\"])\n",
    "print(\"Document BIO Tags: \", test_sample[\"doc_bio_tags\"])\n",
    "print(\"\\n-----------\\n\")\n",
    "print('Type of each dataset:', type(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5680b1c6-ac6f-46ff-8be9-717d268cbdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping doc_bio_tag to integer:\n",
      "\n",
      " {'B': 0, 'I': 1, 'O': 2}\n",
      "\n",
      "Mapping integer to doc_bio_tag:\n",
      "\n",
      " {0: 'B', 1: 'I', 2: 'O'}\n"
     ]
    }
   ],
   "source": [
    "label_list = np.unique(train_sample[\"doc_bio_tags\"])\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print('Mapping doc_bio_tag to integer:\\n\\n',label2id)\n",
    "print('\\nMapping integer to doc_bio_tag:\\n\\n',id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8790832-9e32-42af-91c1-712dafc4797d",
   "metadata": {},
   "source": [
    "## Specifying the base model we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304da274-63cb-4b46-8abd-4972399fdfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilroberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae57ba-ad7a-4982-8ea3-10656430061c",
   "metadata": {},
   "source": [
    "## Getting the same tokenizer that was used in the pre-trained model to preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739b52d-d3d3-4b38-b36d-e977c6d6e234",
   "metadata": {},
   "source": [
    "General Info: \n",
    "\n",
    "A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements.\n",
    "\n",
    "We use add_prefix_space = True to specify that we want to add a space to the first word if there isn’t already one. This lets us treat 'hello' exactly like 'say hello'.\n",
    "\n",
    "Although the documents are already tokenized, we want to make sure that the tokenization matches the one that our pre-trained model is expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf2d836-43af-41d8-8cc0-3ddccf926b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "#check to make sure tokenizer has fast version available\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65fbc7-dcd5-44e5-adda-afb841b6904f",
   "metadata": {},
   "source": [
    "## Convert BIO tags of words to numerical labels (1,2, or 0) corresponding to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343af5e1-1131-4ac5-b4f7-0281f7eafe20",
   "metadata": {},
   "source": [
    "Each word in the document gets converted into a token/multiple tokens. After the word is converted into a token we need to assign that token a classification index corresponding to the BIO tag of the word.  This is done with the function below.\n",
    "\n",
    "see https://huggingface.co/docs/transformers/tasks/token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61029f0c-dac1-4758-be16-840544e46e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words_with_corresponding_labels(sample):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "    #truncation=True to specify to truncate sequences at the maximum length\n",
    "    #is_split_into_words = True to specify that our input is already pre-tokenized (e.g., split into words)\n",
    "    tokenized_inputs = tokenizer(sample[\"document\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    #initialize list to store lists of labels for each sample\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(sample[\"doc_bio_tags\"]):\n",
    "        \n",
    "        #map tokens to their respective word\n",
    "        #word_ids() method gets index of the word that each token comes from\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        #initialize list of labels for each token in a given sample\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            \n",
    "            #set the special tokens, [CLS] and [SEP], to -100.\n",
    "            # we use -100 because it's an index that is ignored in the loss function we will use (cross entropy).\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            #set labels for tokens\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "   \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc24acb-bfee-4c5a-96e3-3cd75acff241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c90c202834c8a8e62ac2befb0557f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c34f65170544daa0c23c05ac815870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fde0d233eea4d079040accdb7efbb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_words_with_corresponding_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fee06f-e0c4-4dea-9256-fa00005956fe",
   "metadata": {},
   "source": [
    "# Finetuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcb2a4-240c-446c-96f7-949ec2fc2af0",
   "metadata": {},
   "source": [
    "Token classification with NLP on HuggingFace info \n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter7/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594e916-caee-45c2-8f15-331327ae7a94",
   "metadata": {},
   "source": [
    "## Data collator to pad inputs sequences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f2a398-cb91-491b-86fe-9ebe1cbcacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamically pad the inputs received, as well as the labels to make them all the same length\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f917850-e54f-492e-a880-1905991fca03",
   "metadata": {},
   "source": [
    "## Track metrics during training\n",
    "\n",
    "To have the Trainer compute a metric every epoch, we will need to define a compute_metrics() function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values.\n",
    "\n",
    "This compute_metrics() function takes the argmax of the logits to convert them to predictions. Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is -100, then pass the results to the metric.compute() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b7c0ac-4088-4d54-8472-0f23a4f0c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eee463a-d93c-4404-a1bb-e551b889dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "    logits, labels = preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    \n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e67d2-9a2b-431a-9af1-c34b3658abb0",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "518bc291-ecd0-4395-8454-4ac01724143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(torch.distributed, \"is_initialized\", lambda : False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0eb9e1f-1ad1-47df-b284-5a83acc92d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea756b1c41a461facc6ec1280ba0917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint,\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id)\n",
    "#need GPU to train\n",
    "#_ = model.to('cuda')\n",
    "\n",
    "#check to make sure we have three labels for outputs\n",
    "print('Number of labels:', model.config.num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10f00f35-131f-42c5-b822-3f29a4901c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}_finetuned_keyword_extract\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type='linear',\n",
    "    weight_decay=0.01,\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e8355-e563-4d2c-9b23-8c2a9956e9a8",
   "metadata": {},
   "source": [
    "## Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0599e0-b152-4dce-93a0-b0103c272a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/375 00:40 < 41:29, 0.15 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2fbe8-9f8a-4f2f-a66a-ca72df1f3b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
